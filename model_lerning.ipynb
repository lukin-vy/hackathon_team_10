{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b872f3fb-2fe9-4a28-a997-fa6bf22916be",
   "metadata": {},
   "source": [
    "Подключим библиотеки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36e768f0-3515-4169-b249-e4dad70f40b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "import shutil\n",
    "import time\n",
    "import xml.etree.ElementTree as et\n",
    "\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from torchvision import models\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee5bbe8-06c3-4ef5-a1a6-8fd44ae9199e",
   "metadata": {},
   "source": [
    "Выбираем, где будут выполняться вычисления.</br>\n",
    "Если есть CUDA выполняем все вычисления на видеокарте. Иначе на процессоре."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f09d7bb-3253-4be1-bf6b-76bf8628d540",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_gpu = torch.cuda.is_available()\n",
    "device = 'cuda' if use_gpu else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bddacc23-6323-4d07-862a-cfbccad6123e",
   "metadata": {},
   "source": [
    "Подготовим Датасеты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3b8d3db-1929-475f-a648-e30c4e68fb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training dataset.\n",
    "train_dataset = ImageFolder(\n",
    "    root='data/train'\n",
    ")\n",
    "# Validation dataset.\n",
    "valid_dataset = ImageFolder(\n",
    "    root='data/test'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b89085-4d96-4eec-88ea-5801056a1fae",
   "metadata": {},
   "source": [
    "Добавим аугментаций - автовыравнивание изображений и автоконтраста (чтобы улучшить обобщающую способность моделей и внизить риск переобучения)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7dadc8d-8bff-4065-b543-86027c3ffa6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "train_dataset.transform = transforms.Compose([\n",
    "    transforms.Resize([70, 70]),\n",
    "    transforms.RandomHorizontalFlip(), # augmentations\n",
    "    transforms.RandomAutocontrast(), # augmentations\n",
    "    transforms.RandomEqualize(), # augmentations\n",
    "    transforms.ToTensor(),\n",
    "    normalize\n",
    "])\n",
    "\n",
    "valid_dataset.transform = transforms.Compose([\n",
    "    transforms.Resize([70, 70]),\n",
    "    transforms.ToTensor(),\n",
    "    normalize\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6da3fef1-9bd8-4fcf-ae6a-56b67b55f8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data loaders.\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=BATCH_SIZE,\n",
    "    shuffle=True\n",
    ")\n",
    "# Validation data loaders.\n",
    "valid_loader = DataLoader(\n",
    "    valid_dataset, batch_size=BATCH_SIZE,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c07cb99-38bc-429a-aea9-98541091c862",
   "metadata": {},
   "source": [
    "Возьмём предобученную нейросеть VGG19.</br>\n",
    "Замораживаем все исходные слои модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75337cd7-13a1-42aa-956c-1bd2b50b11ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vgg_19():\n",
    "    model = models.vgg19(weights=models.VGG19_Weights.IMAGENET1K_V1)\n",
    "    model.classifier[6] = torch.nn.Linear(4096, len(train_dataset.classes))\n",
    "    for param in model.features.parameters():\n",
    "        param.requires_grad = False\n",
    "    for index, block in enumerate(model.features):\n",
    "        if index >= 30:\n",
    "            for param in block.parameters():\n",
    "                param.requires_grad = True\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5712a6-cdb8-46ee-b618-be8043a01558",
   "metadata": {},
   "source": [
    "Для сравнения также воспользуемся моделью google"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c9b780a0-a934-42a0-a298-a1cf404f2fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def google(): # pretrained=True для tensorflow\n",
    "    model = models.googlenet(weights=models.GoogLeNet_Weights.IMAGENET1K_V1)\n",
    "    model.fc = torch.nn.Linear(1024, len(train_dataset.classes))\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = True\n",
    "    model.inception3a.requires_grad = False\n",
    "    model.inception3b.requires_grad = False\n",
    "    model.inception4a.requires_grad = False\n",
    "    model.inception4b.requires_grad = False\n",
    "    model.inception4c.requires_grad = False\n",
    "    model.inception4d.requires_grad = False\n",
    "    model.inception4e.requires_grad = False\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e32bb3-0385-41b2-b46a-0c0bc308be89",
   "metadata": {},
   "source": [
    "Определяем функцию обучения модели "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "93fc8ab5-cc46-44a8-83af-0ddfd1ee7902",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, train_loader, val_loader, epoch=10):\n",
    "    lern_result = pd.DataFrame(columns = ('epochs','loss_train','loss_valid','acc_train','acc_valid'))\n",
    "\n",
    "    for epoch in tqdm(range(epoch)):\n",
    "        lern_new_row = dict()        \n",
    "        losses, equals = [], []\n",
    "        torch.set_grad_enabled(True)\n",
    "\n",
    "        # Train.\n",
    "        model.train()\n",
    "        for i, (image, target) in enumerate(train_loader):\n",
    "            image = image.to(device)\n",
    "            target = target.to(device)\n",
    "            output = model(image)\n",
    "            loss = criterion(output,target)\n",
    "\n",
    "            losses.append(loss.item())\n",
    "            equals.extend(\n",
    "                [x.item() for x in torch.argmax(output, 1) == target])\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        lern_new_row['loss_train'] = round(np.mean(losses), 3)\n",
    "        lern_new_row['acc_train'] = round(np.mean(equals), 3)\n",
    "\n",
    "        losses, equals = [], []\n",
    "        torch.set_grad_enabled(False)\n",
    "\n",
    "        # Validate.\n",
    "        model.eval()\n",
    "        for i , (image, target) in enumerate(valid_loader):\n",
    "            image = image.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "            output = model(image)\n",
    "            loss = criterion(output,target)\n",
    "\n",
    "            losses.append(loss.item())\n",
    "            equals.extend(\n",
    "                [y.item() for y in torch.argmax(output, 1) == target])\n",
    "\n",
    "        lern_new_row['loss_valid'] = round(np.mean(losses), 3)\n",
    "        lern_new_row['acc_valid'] = round(np.mean(equals), 3)\n",
    "        \n",
    "        #Записшем результаты обучения\n",
    "        lern_result.loc[len(lern_result.index)] = [\n",
    "            epoch + 1,\n",
    "            lern_new_row['loss_train'],\n",
    "            lern_new_row['loss_valid'],\n",
    "            lern_new_row['acc_train'],\n",
    "            lern_new_row['acc_valid']\n",
    "        ]\n",
    "\n",
    "    return lern_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db23e2c3-6f16-4434-bad8-6d2c5be61088",
   "metadata": {},
   "source": [
    "Обучение Модели VGG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5c0edda3-6f2a-483d-9a40-d85ce5ecb7a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: vgg_19\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 392.00 MiB. GPU 0 has a total capacity of 1.95 GiB of which 92.19 MiB is free. Process 3181 has 488.12 MiB memory in use. Including non-PyTorch memory, this process has 1.21 GiB memory in use. Of the allocated memory 1.06 GiB is allocated by PyTorch, and 96.15 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-4\u001b[39m)\n\u001b[1;32m      9\u001b[0m model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 11\u001b[0m lern_result \u001b[38;5;241m=\u001b[39m train( model, optimizer, train_loader, valid_loader, epochs_count)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m#Сохраним результаты обучения\u001b[39;00m\n\u001b[1;32m     14\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(model\u001b[38;5;241m.\u001b[39mstate_dict(), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgoogle_ready_model.pth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[8], line 23\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, train_loader, val_loader, epoch)\u001b[0m\n\u001b[1;32m     21\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     22\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 23\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     25\u001b[0m lern_new_row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss_train\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mround\u001b[39m(np\u001b[38;5;241m.\u001b[39mmean(losses), \u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m     26\u001b[0m lern_new_row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124macc_train\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mround\u001b[39m(np\u001b[38;5;241m.\u001b[39mmean(equals), \u001b[38;5;241m3\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/optim/optimizer.py:487\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    482\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    483\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    484\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    485\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0m out \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    488\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    490\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/optim/optimizer.py:91\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     90\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 91\u001b[0m     ret \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     93\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/optim/adam.py:213\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    210\u001b[0m     state_steps: List[Tensor] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    211\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m--> 213\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    214\u001b[0m         group,\n\u001b[1;32m    215\u001b[0m         params_with_grad,\n\u001b[1;32m    216\u001b[0m         grads,\n\u001b[1;32m    217\u001b[0m         exp_avgs,\n\u001b[1;32m    218\u001b[0m         exp_avg_sqs,\n\u001b[1;32m    219\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    220\u001b[0m         state_steps,\n\u001b[1;32m    221\u001b[0m     )\n\u001b[1;32m    223\u001b[0m     adam(\n\u001b[1;32m    224\u001b[0m         params_with_grad,\n\u001b[1;32m    225\u001b[0m         grads,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    243\u001b[0m         found_inf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    244\u001b[0m     )\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/optim/adam.py:153\u001b[0m, in \u001b[0;36mAdam._init_group\u001b[0;34m(self, group, params_with_grad, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps)\u001b[0m\n\u001b[1;32m    143\u001b[0m state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstep\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    144\u001b[0m     torch\u001b[38;5;241m.\u001b[39mzeros(\n\u001b[1;32m    145\u001b[0m         (),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    150\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;241m0.0\u001b[39m, dtype\u001b[38;5;241m=\u001b[39m_get_scalar_dtype())\n\u001b[1;32m    151\u001b[0m )\n\u001b[1;32m    152\u001b[0m \u001b[38;5;66;03m# Exponential moving average of gradient values\u001b[39;00m\n\u001b[0;32m--> 153\u001b[0m state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexp_avg\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros_like(\n\u001b[1;32m    154\u001b[0m     p, memory_format\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mpreserve_format\n\u001b[1;32m    155\u001b[0m )\n\u001b[1;32m    156\u001b[0m \u001b[38;5;66;03m# Exponential moving average of squared gradient values\u001b[39;00m\n\u001b[1;32m    157\u001b[0m state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexp_avg_sq\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros_like(\n\u001b[1;32m    158\u001b[0m     p, memory_format\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mpreserve_format\n\u001b[1;32m    159\u001b[0m )\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 392.00 MiB. GPU 0 has a total capacity of 1.95 GiB of which 92.19 MiB is free. Process 3181 has 488.12 MiB memory in use. Including non-PyTorch memory, this process has 1.21 GiB memory in use. Of the allocated memory 1.06 GiB is allocated by PyTorch, and 96.15 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "model = vgg_19()\n",
    "print('Model: vgg_19\\n')\n",
    "epochs_count = 50\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "model = model.to(device)\n",
    "\n",
    "lern_result = train( model, optimizer, train_loader, valid_loader, epochs_count)\n",
    "\n",
    "#Сохраним результаты обучения\n",
    "torch.save(model.state_dict(), 'vgg_ready_model.pth')\n",
    "#Сохраним обученную модель\n",
    "lern_result.to_csv('vgg_lern_result.csv')\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "print('Обучение окончено')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93426b35-4b01-49e7-bdd0-83eb6cc7aeb8",
   "metadata": {},
   "source": [
    "Обучение Модели google"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cac8ed43-b910-4961-97c0-5ab2d8a4f98c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: google\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 50/50 [13:24<00:00, 16.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Обучение окончено\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "model = google()\n",
    "print('Model: google\\n')\n",
    "epochs_count = 50\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "model = model.to(device)\n",
    "\n",
    "lern_result = train( model, optimizer, train_loader, valid_loader, epochs_count)\n",
    "\n",
    "#Сохраним результаты обучения\n",
    "torch.save(model.state_dict(), 'google_ready_model.pth')\n",
    "#Сохраним обученную модель\n",
    "lern_result.to_csv('google_lern_result.csv')\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "print('Обучение окончено')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
